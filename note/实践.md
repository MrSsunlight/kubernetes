# k8s 

## 部署参考:
[单节点部署](https://mp.weixin.qq.com/s/NdxulImlipD-zWUY2eUHYw)

初始化:

```shell
[root@master ~]# kubeadm init --pod-network-cidr=192.168.101.0/16
I1213 17:05:28.509486   96937 version.go:255] remote version is much newer: v1.23.0; falling back to: stable-1.22
[init] Using Kubernetes version: v1.22.4
[preflight] Running pre-flight checks
...

[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.101.135:6443 --token p8gwg9.n4h8kfjymqjq2stg \
	--discovery-token-ca-cert-hash sha256:e4bcf02565b48d371fc7adcfabac3754667bb0909b6cd3213fb10d9ca05ce395 
```


```shell
[root@master ~]# mkdir -p $HOME/.kube
[root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@master ~]# export KUBECONFIG=/etc/kubernetes/admin.conf
[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
serviceaccount/flannel created
...

[root@master ~]# kubectl get node
NAME     STATUS   ROLES                  AGE   VERSION
master   Ready    control-plane,master   10m   v1.22.4
[root@master ~]# kubectl get pod -n kube-system
NAME                             READY   STATUS              RESTARTS   AGE
coredns-78fcd69978-ld6jp         0/1     ContainerCreating   0          11m
coredns-78fcd69978-mvl9v         0/1     ContainerCreating   0          11m
etcd-master                      1/1     Running             0          11m
kube-apiserver-master            1/1     Running             0          11m
kube-controller-manager-master   1/1     Running             0          11m
kube-proxy-w99k6                 1/1     Running             0          11m
kube-scheduler-master            1/1     Running             0          11m
[root@master ~]# 
[root@master ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
etcd-0               Healthy     {"health":"true","reason":""}                                                                 
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Healthy     ok                                                                                            
```

异常处理: 出现这种情况，是/etc/kubernetes/manifests/下的kube-controller-manager.yaml和kube-scheduler.yaml设置的默认端口是0 (--port=0) 导致的，解决方式是注释掉对应的port即可

```shell
[root@master ~]# vim /etc/kubernetes/manifests/kube-scheduler.yaml 
[root@master ~]# vim /etc/kubernetes/manifests/kube-controller-manager.yaml 
[root@master ~]# systemctl restart kubelet.service
[root@master ~]# 
[root@master ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
scheduler            Healthy   ok                              
controller-manager   Healthy   ok                              
etcd-0               Healthy   {"health":"true","reason":""}   
[root@master ~]# 

```

```shell
[root@master ~]# kubectl get nodes
NAME     STATUS   ROLES                  AGE   VERSION
master   Ready    control-plane,master   12m   v1.22.4
[root@master ~]# kubectl describe node master | grep Taints
Taints:             node-role.kubernetes.io/master:NoSchedule
```

默认master节点是不跑业务pod的，我们暂时只有一个node，所以先去掉这个Taint：

```shell
[root@master ~]# kubectl taint nodes master node-role.kubernetes.io/master-
node/master untainted
[root@master ~]# kubectl describe node master | grep Taints
Taints:             <none>
[root@master ~]# 

```

master 上添加taint:

`kubectl taint nodes master1 node-role.kubernetes.io/master=:NoSchedule`

取消master1 上的 taint:

`kubectl taint nodes master1 node-role.kubernetes.io/master-`





[集群部署](https://mp.weixin.qq.com/s/NdxulImlipD-zWUY2eUHYw)


## get token : `kubeadm token list`
## get ca-cert-hash: `openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null |openssl dgst -sha256 -hex | sed 's/^.* //'`

## init

`kubeadm join 192.168.192.128:6443 --token 6eoyps.9tmgii4f9jn0lwyt --discovery-token-ca-cert-hash sha256:5021e9aa11cec63dca94e156b0eff371cf76462dca81df9a6279ee1cc375f4d6`

`kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.22.0 --apiserver-advertise-address=192.168.192.128`

`kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.22.0 --apiserver-advertise-address=192.168.192.128 --pod-network-cidr=192.168.101.0/16`

初始化失败:

```shell
[root@master ~]# kubeadm init --pod-network-cidr=192.168.101.0/16
I1213 16:50:27.921394   96094 version.go:255] remote version is much newer: v1.23.0; falling back to: stable-1.22
[init] Using Kubernetes version: v1.22.4
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

```
解决办法:
`echo 1 > /proc/sys/net/ipv4/ip_forward`



## watch

`watch kubectl get pods -n calico-system`

`watch kubectl get nodes`


## cluster info

`kubectl cluster-info`
```shell
[root@vm1 ~]# kubectl cluster-info
Kubernetes control plane is running at https://192.168.192.128:6443
CoreDNS is running at https://192.168.192.128:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

### https://192.168.192.128:6443/ web error
```json
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {
    
  },
  "code": 403
}
```  
生成client-certificate-data

`grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt`
生成client-key-data

`grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key`
生成p12

`openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"`
kubecfg.p12就是生成的个人证书

浏览器导入证书,然后关闭浏览器，重新登录后通过token登录.
```shell
[root@master ~]# kubectl get secret -n kube-system |grep admin|awk '{print $1}'
dashboard-admin-token-swhrz
[root@master ~]# kubectl describe secret/dashboard-admin-token-swhrz -n kube-system
Name:         dashboard-admin-token-swhrz
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin
              kubernetes.io/service-account.uid: d59adb94-f4ae-4180-8b69-4cd8f2c2e5f4

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6Ikp2bV9pZmNIR0xqLUxRREd3QlRzNU1pdnBkYnMxTXRlWG15alBidW0xNTAifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tc3docnoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZDU5YWRiOTQtZjRhZS00MTgwLThiNjktNGNkOGYyYzJlNWY0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.K0td6E4SjkgjQvQ9ucxecNkhEFmKhOtrwlgNpq2yJZvdm_MOuSAl4P7J7PGkFf6UoEXJ1jgk4eyMeLR9eJZ8KV9rwTt5U-snH_dGetejeofI6pk0aIHWyIq7KnuKbH8m_Q8Ok4eDatOW06_Q8hs0ZYktZ-J5uPytuS0jUuG47pxRTu5PwFtR-svypE7mP7Sz1rORyT7wultWysvA1zFS93DhRlIBJwbvv2UQI9cDbJcXl3x-HItPpZaPFrGqKTRZoXvAxoaUCm7BhPm9XO0xhE5H_ItGO09IZnb_Ib3kCF-W9-9fITPBIo4vaF9Z7m7nbaz9StID2RrCWV7iP1ysgg
```







